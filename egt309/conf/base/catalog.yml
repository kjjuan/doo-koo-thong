# Here you can define all your Datasets by using simple YAML syntax.
#
# Documentation for this file format can be found in "The Data Catalog"
# Link: https://docs.kedro.org/en/stable/data/data_catalog.html

# bmarket.db was changed to csv
df_raw:
  filepath: data/01_raw/bmarket.csv
  type: pandas.CSVDataset

# cleaned and feature-engineered data
df_cleaned:
  filepath: data/03_primary/df_cleaned.csv
  type: pandas.CSVDataset

# dataset used for splitting and initial processing
df_prep:
  filepath: "data/04_feature/df_prep.csv"
  type: pandas.CSVDataset

# unprocessed split data (used as input to ImbPipeline)
X_train:
  filepath: "data/04_feature/X_train.csv"
  type: pandas.CSVDataset

X_test:
  filepath: "data/04_feature/X_test.csv"
  type: pandas.CSVDataset

y_train_unprocessed:
  filepath: "data/04_feature/y_train.csv"
  type: pandas.CSVDataset

y_test_unprocessed: 
  filepath: "data/04_feature/y_test.csv"
  type: pandas.CSVDataset

# processed split data (output of preprocessing node) 
X_train_processed:
  filepath: "data/05_model_input/X_train_processed.csv"
  type: pandas.CSVDataset

X_test_processed:
  filepath: "data/05_model_input/X_test_processed.csv"
  type: pandas.CSVDataset

# Column Transformer preprocessor
preprocessor:
  filepath: "data/05_model_input/preprocessor.pkl"
  type: pickle.PickleDataset

processed_features:
  filepath: "data/04_feature/processed_features.json"
  type: json.JSONDataset

# models
trained_logreg_model:
  filepath: data/06_models/logreg_model.pkl
  type: pickle.PickleDataset

trained_rf_model:
  filepath: data/06_models/random_forest_model.pkl
  type: pickle.PickleDataset

trained_gb_model:
  filepath: data/06_models/gradient_boosting_model.pkl
  type: pickle.PickleDataset

trained_xgb_model:
  filepath: data/06_models/xgboost_model.pkl
  type: pickle.PickleDataset

# Evaluation Output
model_evaluation_metrics:
  filepath: "data/08_reporting/model_evaluation_metrics.csv"
  type: pandas.CSVDataset

