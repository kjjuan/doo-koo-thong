# conf/base/parameters.yml

#Configured to split training and test data to 80% and 20% respectively
data_split_options:
  test_size: 0.2          # 20% of data used for testing
  random_state: 42        # fixes randomness so results are repeatable

#All features used for model training
features:
  categorical:
    - Occupation
    - Marital Status
    - Education Level
    - Credit Default
    - Personal Loan
    - Contact Method
  numeric:
    - Age
    - Campaign Calls
    - PreviouslyContacted
    - WasContactedBefore

# Machine Learning model hyperparameters used for each model
# 'gradient_boosting' uses a cautious learning rate (0.05) over 300 boosting stages, with shallow trees (max_depth=3) to prevent overfitting.
# 'xgboost' (Extreme Gradient Boosting) is highly optimized with 500 trees,
# a small learning rate (0.03), medium depth (5), and subsampling (0.8)
# for both rows and features to further enhance generalization.
# It uses 'logloss' as the evaluation metric.
model_hyperparameters:
#'logreg' is the 1st model, the logistic regession model 
  logreg:
    max_iter: 500
    class_weight: balanced
#'random_forest' is the 2nd model with 
  random_forest:
    n_estimators: 300        # number of trees in the forest
    max_depth: 12            # maximum depth of each tree (controls overfitting)
    min_samples_split: 5     # minimum samples needed to split a node
    class_weight: balanced   # balance yes/no classes in training
  gradient_boosting:
    learning_rate: 0.05      # step size for each boosting iteration
    n_estimators: 300        # number of boosting stages (trees)
    max_depth: 3             # shallow trees to reduce overfitting
  xgboost:
    n_estimators: 500
    learning_rate: 0.03
    max_depth: 5
    subsample: 0.8
    colsample_bytree: 0.8
    eval_metric: logloss

evaluation_thresholds: [0.7, 0.5, 0.3, 0.1]
