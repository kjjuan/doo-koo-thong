# options for how to split train / test data
data_split_options:
  test_size: 0.2          # 20% of data used for testing
  random_state: 42        # fixes randomness so results are repeatable

# which columns are treated as categorical vs numeric
features:
  categorical:
    - Age Group          # binned age category
    - Occupation         # job type
    - Marital Status     # single / married / divorced / etc.
    - Education Level    # highest education
    - Credit Default     # has credit default: yes/no/unknown
    - Personal Loan      # personal loan status
    - Contact Method     # cellular / telephone
    - Housing Loan       # housing loan status
  numeric:
    - Campaign Calls     # number of calls in current campaign
    - PreviouslyContacted # days since previous contact (0 if never)
    - WasContactedBefore # flag if contacted before (0/1)

# hyperparameters for each ML model
model_hyperparameters:
  logreg:
    max_iter: 500            # maximum optimisation steps for convergence
    class_weight: balanced   # gives more weight to minority class
  random_forest:
    n_estimators: 300        # number of trees in the forest
    max_depth: 12            # maximum depth of each tree (controls overfitting)
    min_samples_split: 5     # minimum samples needed to split a node
    class_weight: balanced   # balance yes/no classes in training
  gradient_boosting:
    learning_rate: 0.05      # step size for each boosting iteration
    n_estimators: 300        # number of boosting stages (trees)
    max_depth: 3             # shallow trees to reduce overfitting
  xgboost:
    n_estimators: 600        # number of boosted trees
    learning_rate: 0.01      # smaller learning rate for smoother learning
    max_depth: 5             # tree depth (model complexity)
    subsample: 0.9           # uses 90% of rows per tree (prevents overfitting)
    colsample_bytree: 0.9    # uses 90% of features per tree (prevents overfitting)
    eval_metric: logloss     # loss function used during training
    gamma: 1.5               # minimum loss reduction to make a split (prunes weak splits)
    scale_pos_weight: 4      # up-weights positive class to handle imbalance
    reg_lambda: 10           # L2 regularisation (shrinks large weights)

# thresholds used to turn predicted probability into class yes/no
evaluation_thresholds: [0.28, 0.32, 0.5]
