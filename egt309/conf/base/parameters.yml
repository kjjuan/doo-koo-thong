# conf/base/parameters.yml

#Configured to split training and test data to 80% and 20% respectively
data_split_options:
  test_size: 0.2          # 20% of data used for testing
  random_state: 42        # fixes randomness so results are repeatable

#All features used for model training
features:
  categorical:
    - Occupation
    - Marital Status
    - Education Level
    - Credit Default
    - Personal Loan
    - Contact Method
  numeric:
    - Age
    - Campaign Calls
    - PreviouslyContacted
    - WasContactedBefore

# Machine Learning model hyperparameters used for each model
# 'gradient_boosting' uses a cautious learning rate (0.05) over 300 boosting stages, with shallow trees (max_depth=3) to prevent overfitting.
# 'xgboost' (Extreme Gradient Boosting) is highly optimized with 500 trees,
# a small learning rate (0.03), medium depth (5), and subsampling (0.8)
# for both rows and features to further enhance generalization.
# It uses 'logloss' as the evaluation metric.
model_hyperparameters:
#'logreg' is the 1st model, the logistic regession model 
  logreg:
    max_iter: 500
    class_weight: balanced
#'random_forest' is the 2nd model with 
  random_forest:
    n_estimators: 300        # number of trees in the forest
    max_depth: 12            # maximum depth of each tree (controls overfitting)
    min_samples_split: 5     # minimum samples needed to split a node
    class_weight: balanced   # balance yes/no classes in training
  gradient_boosting:
    learning_rate: 0.05      # step size for each boosting iteration
    n_estimators: 300        # number of boosting stages (trees)
    max_depth: 3             # shallow trees to reduce overfitting
  xgboost:
    n_estimators: 600        # number of boosted trees
    learning_rate: 0.01      # smaller learning rate for smoother learning
    max_depth: 5             # tree depth (model complexity)
    subsample: 0.9           # uses 90% of rows per tree (prevents overfitting)
    colsample_bytree: 0.9    # uses 90% of features per tree (prevents overfitting)
    eval_metric: logloss     # loss function used during training
    gamma: 1.5               # minimum loss reduction to make a split (prunes weak splits)
    scale_pos_weight: 4      # up-weights positive class to handle imbalance
    reg_lambda: 10           # L2 regularisation (shrinks large weights)

# thresholds used to turn predicted probability into class yes/no
evaluation_thresholds: [0.28, 0.32, 0.5]
